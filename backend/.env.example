OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-4o-mini

# LLM provider: openai | grok | ollama | openai_compatible (e.g. vLLM)
LLM_PROVIDER=openai
# For grok (xAI): get key at https://console.x.ai
XAI_API_KEY=
# For grok: grok-4-1-fast-non-reasoning, grok-4-1-fast-reasoning, grok-4, etc.
# For ollama/openai_compatible: base URL (default http://localhost:11434)
LLM_BASE_URL=http://localhost:11434
# For ollama: deepseek-r1; for grok: grok-4-1-fast-non-reasoning
LLM_MODEL=

# Lower = stick to rulebook (less hallucination). Default 0.3. Use 0.1â€“0.2 for local models if they still invent.
# LLM_TEMPERATURE=0.3
# Optional: top_p (leave unset to use API default)
# LLM_TOP_P=

# Optional: log GM reply hash and prefix for bias/canon auditing (no PII)
LOG_GM_RESPONSES=false
LOG_GM_RESPONSE_PREFIX_LEN=200

# Optional: root dir containing System_Summary, AllBookPages-FullBook, AllBookTables-csv. If unset, uses reference/TTRPG_DRD (or any TTRPG* folder).
# RAG_SOURCE_DIR=reference/TTRPG_DRD
FAISS_PATH=./faiss_drd
# Set to 1 (or true/yes) and restart to force RAG index rebuild once (e.g. after changing source MDs/CSVs)
RAG_FORCE_REBUILD=
RAG_TOP_K=12
CORS_WILDCARD=true
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173,http://localhost:3000,http://127.0.0.1:5500
